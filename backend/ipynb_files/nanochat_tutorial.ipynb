{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Building Your Own ChatGPT: A Complete Guide\n",
    "\n",
    "## Welcome, Future AI Engineers!\n",
    "\n",
    "Have you ever wondered how ChatGPT works? In this notebook, we're going to learn how to build our own ChatGPT-like AI from scratch using a project called **nanochat** by Andrej Karpathy (former director of AI at Tesla and OpenAI).\n",
    "\n",
    "### What You'll Learn:\n",
    "1. What are Large Language Models (LLMs)?\n",
    "2. How does a chatbot understand and generate text?\n",
    "3. The complete pipeline: from raw text to a working AI assistant\n",
    "4. Hands-on experience with real AI code\n",
    "\n",
    "### Prerequisites:\n",
    "- Basic understanding of Python\n",
    "- Curiosity about AI!\n",
    "- No advanced math required - we'll explain everything\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Table of Contents\n",
    "1. Introduction to Language Models\n",
    "2. Understanding the Architecture\n",
    "3. The Training Pipeline\n",
    "4. Tokenization: Teaching AI to Read\n",
    "5. Building the Transformer Model\n",
    "6. Training Your Model\n",
    "7. Making Your AI Chat\n",
    "8. Evaluation and Testing\n",
    "9. Next Steps\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: What is a Language Model?\n",
    "\n",
    "## üß† The Big Idea\n",
    "\n",
    "Imagine you're writing a text message: \"I'm going to the...\"\n",
    "\n",
    "Your brain can predict what comes next: \"store\", \"park\", \"movies\", etc. A **language model** does the same thing!\n",
    "\n",
    "### Definition:\n",
    "A **Large Language Model (LLM)** is a computer program that:\n",
    "1. Reads a lot of text (books, websites, articles)\n",
    "2. Learns patterns in how people write\n",
    "3. Uses those patterns to predict what word comes next\n",
    "4. Generates human-like text by predicting one word at a time\n",
    "\n",
    "### How Big is \"Large\"?\n",
    "- **nanochat (our version)**: ~500 million to 2 billion parameters\n",
    "- **GPT-2** (2019): 1.5 billion parameters\n",
    "- **GPT-3** (2020): 175 billion parameters\n",
    "- **GPT-4** (2023): Estimated 1+ trillion parameters\n",
    "\n",
    "A **parameter** is like a tiny piece of knowledge the model learns. More parameters = more knowledge capacity!\n",
    "\n",
    "### The Magic Formula:\n",
    "```\n",
    "Text Input ‚Üí Model ‚Üí Predicted Next Word\n",
    "```\n",
    "\n",
    "By repeating this over and over, the model can write entire paragraphs, stories, or even code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Understanding the Transformer Architecture\n",
    "\n",
    "## üèóÔ∏è The Building Blocks\n",
    "\n",
    "ChatGPT and nanochat are built using something called a **Transformer**. Think of it as a sophisticated pattern-matching machine.\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "#### 1. **Tokens** (The Words)\n",
    "Before the model can read text, it breaks it into \"tokens\" - small pieces of text.\n",
    "- Example: \"Hello, world!\" ‚Üí [\"Hello\", \",\", \" world\", \"!\"]\n",
    "\n",
    "#### 2. **Embeddings** (The Meaning)\n",
    "Each token gets converted into a list of numbers (a vector) that represents its meaning.\n",
    "- Similar words have similar numbers\n",
    "- Example: \"cat\" and \"dog\" have similar embeddings because they're both animals\n",
    "\n",
    "#### 3. **Attention Mechanism** (The Focus)\n",
    "This is the most important innovation! When processing a word, the model looks at ALL other words to understand context.\n",
    "\n",
    "Example sentence: \"The animal didn't cross the street because it was too tired.\"\n",
    "- When processing \"it\", attention helps the model know \"it\" refers to \"animal\", not \"street\"\n",
    "\n",
    "#### 4. **Layers** (The Depth)\n",
    "The model has many layers stacked on top of each other. Each layer:\n",
    "- Looks at the text from a different perspective\n",
    "- Extracts more complex patterns\n",
    "- Builds deeper understanding\n",
    "\n",
    "Our nanochat model has 20-34 layers!\n",
    "\n",
    "#### 5. **Prediction Head** (The Output)\n",
    "At the end, the model outputs probabilities for what word comes next:\n",
    "- \"store\": 35% probability\n",
    "- \"park\": 25% probability\n",
    "- \"movies\": 20% probability\n",
    "- etc.\n",
    "\n",
    "### Visual Flow:\n",
    "```\n",
    "Input Text\n",
    "    ‚Üì\n",
    "Tokenization (break into pieces)\n",
    "    ‚Üì\n",
    "Embeddings (convert to numbers)\n",
    "    ‚Üì\n",
    "Layer 1 (attention + processing)\n",
    "    ‚Üì\n",
    "Layer 2 (attention + processing)\n",
    "    ‚Üì\n",
    "    ...\n",
    "    ‚Üì\n",
    "Layer 20 (attention + processing)\n",
    "    ‚Üì\n",
    "Prediction (what comes next?)\n",
    "    ‚Üì\n",
    "Output Text\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: The Complete Training Pipeline\n",
    "\n",
    "## üéì From Zero to ChatGPT\n",
    "\n",
    "Training an AI chatbot happens in several stages. Think of it like teaching a child:\n",
    "\n",
    "### Stage 1: Learning to Read (Tokenization)\n",
    "**What happens:** We teach the model how to break text into pieces (tokens).\n",
    "\n",
    "**Why it matters:** Just like you learned the alphabet before reading, the model needs to learn its \"alphabet\" of text pieces.\n",
    "\n",
    "**Example:**\n",
    "- Input: \"I love pizza!\"\n",
    "- Tokens: [\"I\", \" love\", \" pizza\", \"!\"]\n",
    "- Token IDs: [314, 1842, 16462, 0]\n",
    "\n",
    "---\n",
    "\n",
    "### Stage 2: Base Training (Pretraining)\n",
    "**What happens:** The model reads millions of documents and learns to predict the next word.\n",
    "\n",
    "**Dataset:** FineWeb - a huge collection of high-quality web pages\n",
    "- Size: ~90 billion characters (for our speedrun version)\n",
    "- That's like reading 45,000 novels!\n",
    "\n",
    "**Training Process:**\n",
    "1. Show the model text: \"The sky is\"\n",
    "2. Model guesses: \"green\" (wrong!)\n",
    "3. Tell the model the correct answer: \"blue\"\n",
    "4. Model adjusts its internal parameters\n",
    "5. Repeat billions of times!\n",
    "\n",
    "**Result:** A model that can predict text, but doesn't know how to have conversations yet.\n",
    "\n",
    "---\n",
    "\n",
    "### Stage 3: Midtraining\n",
    "**What happens:** We teach the model about conversations and special tasks.\n",
    "\n",
    "**New Skills:**\n",
    "- Understanding chat format (user says something, assistant responds)\n",
    "- Learning to use tools (like a calculator)\n",
    "- Answering multiple-choice questions\n",
    "- Developing a personality\n",
    "\n",
    "**Example Training Data:**\n",
    "```\n",
    "User: What is 2+2?\n",
    "Assistant: 2+2 equals 4.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Stage 4: Supervised Fine-tuning (SFT)\n",
    "**What happens:** We show the model examples of really good conversations.\n",
    "\n",
    "**Goal:** Make the model:\n",
    "- More helpful\n",
    "- More accurate\n",
    "- Better at following instructions\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "User: Explain photosynthesis to a 5-year-old.\n",
    "Assistant: Plants are like tiny chefs! They use sunlight as their energy \n",
    "to cook up food from air and water. The recipe makes food for the plant \n",
    "and releases oxygen that we breathe. Cool, right?\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Stage 5: Reinforcement Learning (Optional)\n",
    "**What happens:** The model practices tasks and gets rewarded for correct answers.\n",
    "\n",
    "**How it works:**\n",
    "1. Give the model a math problem\n",
    "2. Model generates an answer\n",
    "3. Check if the answer is correct\n",
    "4. Give a reward (like a gold star!) if correct\n",
    "5. Model learns to maximize rewards\n",
    "\n",
    "**Result:** Better performance on specific tasks like math problems!\n",
    "\n",
    "---\n",
    "\n",
    "### The Timeline\n",
    "- **Tokenization:** 5-10 minutes\n",
    "- **Base Training:** 2-3 hours (the longest part!)\n",
    "- **Midtraining:** 20-30 minutes\n",
    "- **SFT:** 10-15 minutes\n",
    "- **RL:** 10-15 minutes\n",
    "\n",
    "**Total for speedrun version:** About 4 hours and $100 on powerful GPUs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Part 4: Let's Get Started - Setup\n\n## üöÄ Your Local Environment\n\nNow let's get hands-on! This notebook is configured to run on your local Ubuntu server with the GB10 Blackwell GPU.\n\n### Your Hardware:\n- **GPU**: GB10 Blackwell (128GB VRAM)\n- **Location**: 192.168.219.45\n- **nanochat**: Pre-installed at `/var/www/gpt2/nanochat`\n\n### What You Can Do:\n- Explore the nanochat code and architecture\n- Run training experiments with your powerful GPU\n- Train full models (speedrun takes ~4 hours, costs ~$100 in compute)\n- Experiment with different configurations\n\nLet's verify your environment is ready!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check your local environment\nimport sys\nimport os\nimport subprocess\nimport torch\n\nprint(\"üñ•Ô∏è  Environment Check\")\nprint(\"=\"*60)\n\n# Python version\nprint(f\"\\nPython: {sys.version}\")\n\n# Check PyTorch\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    print(f\"CUDA version: {torch.version.cuda}\")\n    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n\n    # Get GPU info\n    try:\n        gpu_info = subprocess.check_output(\n            ['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'],\n            text=True\n        ).strip()\n        print(f\"\\nüéÆ GPU: {gpu_info}\")\n\n        # Check if it's the GB10 Blackwell\n        for i in range(torch.cuda.device_count()):\n            print(f\"   Device {i}: {torch.cuda.get_device_name(i)}\")\n            total_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n            print(f\"   Memory: {total_memory:.1f} GB\")\n    except Exception as e:\n        print(f\"GPU info: {e}\")\n\n# Check nanochat installation\nnanochat_path = \"/var/www/gpt2/nanochat\"\nif os.path.exists(nanochat_path):\n    print(f\"\\n‚úÖ nanochat found at: {nanochat_path}\")\n    sys.path.insert(0, nanochat_path)\nelse:\n    print(f\"\\n‚ö†Ô∏è  nanochat not found at {nanochat_path}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚úÖ Environment ready for AI training!\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Navigate to nanochat directory\nimport os\nimport sys\n\nnanochat_path = \"/var/www/gpt2/nanochat\"\nprint(f\"üìÅ Using nanochat installation at: {nanochat_path}\\n\")\n\nif os.path.exists(nanochat_path):\n    os.chdir(nanochat_path)\n    # Add to Python path\n    if nanochat_path not in sys.path:\n        sys.path.insert(0, nanochat_path)\n\n    print(\"‚úÖ Changed to nanochat directory\")\n    print(f\"   Current directory: {os.getcwd()}\\n\")\n\n    print(\"üìÇ Directory contents:\")\n    import subprocess\n    result = subprocess.run(['ls', '-lh'], capture_output=True, text=True)\n    print(result.stdout)\nelse:\n    print(f\"‚ùå nanochat not found at {nanochat_path}\")\n    print(\"   Please ensure nanochat is installed\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check nanochat dependencies\nprint(\"üì¶ Checking nanochat environment\\n\")\nprint(\"=\"*60)\n\nimport sys\nimport importlib.util\n\n# nanochat uses a virtual environment\nvenv_path = \"/var/www/gpt2/nanochat/.venv\"\nprint(f\"Virtual environment: {venv_path}\\n\")\n\n# Key dependencies to check\ndependencies = [\n    ('torch', 'PyTorch'),\n    ('tiktoken', 'Tokenization'),\n    ('tqdm', 'Progress bars'),\n    ('numpy', 'Numerical computing'),\n]\n\nprint(\"Checking installed packages:\\n\")\nall_installed = True\n\nfor module_name, description in dependencies:\n    spec = importlib.util.find_spec(module_name)\n    if spec is not None:\n        module = importlib.import_module(module_name)\n        version = getattr(module, '__version__', 'unknown')\n        print(f\"  ‚úÖ {description:20s} ({module_name}): {version}\")\n    else:\n        print(f\"  ‚ùå {description:20s} ({module_name}): NOT FOUND\")\n        all_installed = False\n\nprint(\"\\n\" + \"=\"*60)\n\nif all_installed:\n    print(\"‚úÖ All dependencies are installed!\")\n    print(\"\\nTo activate the nanochat environment in a terminal:\")\n    print(f\"   cd /var/www/gpt2/nanochat\")\n    print(f\"   source .venv/bin/activate\")\nelse:\n    print(\"‚ö†Ô∏è  Some dependencies missing. Install with:\")\n    print(f\"   cd /var/www/gpt2/nanochat\")\n    print(f\"   source .venv/bin/activate\")\n    print(f\"   pip install -e .\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Exploring the Code Structure\n",
    "\n",
    "## üóÇÔ∏è Understanding the Project\n",
    "\n",
    "Let's explore what files make up nanochat. Each file has a specific job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Let's look at the structure of the project\nprint(\"üìÅ Project Structure:\\n\")\nprint(\"=\"*60)\n\nimport os\nfrom pathlib import Path\n\n# Ensure we're in the nanochat directory\nnanochat_path = Path(\"/var/www/gpt2/nanochat\")\nif nanochat_path.exists():\n    os.chdir(nanochat_path)\n\ndef print_tree(directory, prefix=\"\", max_depth=2, current_depth=0):\n    \"\"\"Print a nice tree structure of the project\"\"\"\n    if current_depth >= max_depth:\n        return\n\n    try:\n        entries = sorted(Path(directory).iterdir(), key=lambda x: (not x.is_dir(), x.name))\n        entries = [e for e in entries if not e.name.startswith('.') and e.name not in ['__pycache__', 'uv.lock', '.venv', 'logs']]\n\n        for i, entry in enumerate(entries):\n            is_last = i == len(entries) - 1\n            current_prefix = \"‚îî‚îÄ‚îÄ \" if is_last else \"‚îú‚îÄ‚îÄ \"\n            print(f\"{prefix}{current_prefix}{entry.name}\")\n\n            if entry.is_dir():\n                extension = \"    \" if is_last else \"‚îÇ   \"\n                print_tree(entry, prefix + extension, max_depth, current_depth + 1)\n    except PermissionError:\n        pass\n\nprint_tree(nanochat_path)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nüìù Key Directories:\")\nprint(\"   nanochat/ - Core model code (the brain!)\")\nprint(\"   scripts/  - Training and evaluation scripts\")\nprint(\"   tasks/    - Test benchmarks (how smart is our AI?)\")\nprint(\"   tests/    - Unit tests (making sure code works)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Understanding Tokenization\n",
    "\n",
    "## üî§ How Computers Read Text\n",
    "\n",
    "Remember: computers only understand numbers, not words! Tokenization is how we convert text to numbers.\n",
    "\n",
    "### The Process:\n",
    "1. **Byte Pair Encoding (BPE):** The algorithm used\n",
    "2. Start with individual characters: [\"H\", \"e\", \"l\", \"l\", \"o\"]\n",
    "3. Find common pairs: \"ll\" appears together often\n",
    "4. Merge them: [\"H\", \"e\", \"ll\", \"o\"]\n",
    "5. Repeat to build a vocabulary of ~65,536 tokens\n",
    "\n",
    "### Why This Matters:\n",
    "- Efficient: Common words = 1 token, rare words = multiple tokens\n",
    "- Flexible: Can handle any text, even words the model has never seen\n",
    "- Smart: Related words often share token pieces\n",
    "\n",
    "Let's see tokenization in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore tokenization with a simple example\n",
    "print(\"üî§ Tokenization Demo\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# For this demo, we'll use a simpler tokenizer that's already available\n",
    "# The real nanochat trains its own tokenizer, but this shows the concept!\n",
    "\n",
    "try:\n",
    "    import tiktoken\n",
    "    \n",
    "    # Load GPT-2's tokenizer (similar to what nanochat does)\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    \n",
    "    # Test sentences\n",
    "    test_sentences = [\n",
    "        \"Hello, world!\",\n",
    "        \"Artificial intelligence is amazing!\",\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"I love programming in Python!\"\n",
    "    ]\n",
    "    \n",
    "    for sentence in test_sentences:\n",
    "        # Encode: text -> token IDs\n",
    "        tokens = enc.encode(sentence)\n",
    "        \n",
    "        # Decode each token to see what it represents\n",
    "        token_strings = [enc.decode([t]) for t in tokens]\n",
    "        \n",
    "        print(f\"\\nüìù Original text: \\\"{sentence}\\\"\")\n",
    "        print(f\"   Number of tokens: {len(tokens)}\")\n",
    "        print(f\"   Token IDs: {tokens}\")\n",
    "        print(f\"   Token pieces: {token_strings}\")\n",
    "        \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"\\nüí° Notice how:\")\n",
    "    print(\"   - Common words like 'the' are single tokens\")\n",
    "    print(\"   - Spaces are often attached to words\")\n",
    "    print(\"   - Punctuation can be separate tokens\")\n",
    "    print(\"   - The same word always gets the same token IDs\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Note: Full tokenization demo requires additional setup.\")\n",
    "    print(f\"Concept: Text is broken into small pieces and converted to numbers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì Key Takeaway:\n",
    "\n",
    "Every word you type gets converted to numbers before the AI can process it. The model learns to recognize patterns in these numbers, just like you learned to recognize patterns in letters to read words!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 7: The GPT Model Architecture\n",
    "\n",
    "## üèóÔ∏è Building the Brain\n",
    "\n",
    "Now let's look at the actual code that defines the model! This is where the magic happens.\n",
    "\n",
    "### Key Concepts in the Code:\n",
    "\n",
    "1. **Embeddings:** Convert token IDs to meaningful numbers\n",
    "2. **Attention Layers:** Let tokens \"talk\" to each other\n",
    "3. **Feed-Forward Networks:** Process the information\n",
    "4. **Layer Normalization:** Keep numbers in a good range\n",
    "5. **Residual Connections:** Help information flow through deep networks\n",
    "\n",
    "Let's peek at the actual model code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Let's look at key parts of the GPT model\nprint(\"üß† GPT Model Structure\\n\")\nprint(\"=\"*60)\n\nimport os\nfrom pathlib import Path\n\n# Ensure we're in the right directory\nnanochat_path = Path(\"/var/www/gpt2/nanochat\")\ngpt_file = nanochat_path / \"nanochat\" / \"gpt.py\"\n\nif gpt_file.exists():\n    # Read the GPT model file\n    with open(gpt_file, 'r') as f:\n        gpt_code = f.read()\n\n    # Show the GPTConfig (the model's settings)\n    print(\"\\nüìã Model Configuration:\\n\")\n    config_start = gpt_code.find('@dataclass\\nclass GPTConfig:')\n    if config_start != -1:\n        config_end = gpt_code.find('\\n\\n', config_start)\n        print(gpt_code[config_start:config_end])\n    else:\n        print(\"GPTConfig class definition found in gpt.py\")\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"\\nüí° What these settings mean:\")\n    print(\"   sequence_len: How many tokens the model can look at once (1024)\")\n    print(\"   vocab_size: How many different tokens it knows (~50,000)\")\n    print(\"   n_layer: How many layers deep (12 for small, 20-34 for nanochat)\")\n    print(\"   n_head: Number of attention 'heads' - different perspectives (6)\")\n    print(\"   n_embd: Size of each embedding vector (768 numbers per token)\")\n\n    print(\"\\nüéØ The bigger these numbers, the smarter (and slower) the model!\")\nelse:\n    print(f\"‚ö†Ô∏è  Could not find {gpt_file}\")\n    print(\"   Make sure nanochat is properly installed\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Let's create a tiny example model to understand the structure\nprint(\"üî¨ Creating a Mini-GPT Model\\n\")\nprint(\"=\"*60)\n\nimport sys\nimport os\n\n# Add nanochat to path\nnanochat_path = \"/var/www/gpt2/nanochat\"\nif nanochat_path not in sys.path:\n    sys.path.insert(0, nanochat_path)\n\ntry:\n    import torch\n    import torch.nn as nn\n\n    # Create a simple version of the attention mechanism\n    class SimpleAttention(nn.Module):\n        \"\"\"A simplified attention mechanism for learning\"\"\"\n        def __init__(self, embed_size):\n            super().__init__()\n            self.embed_size = embed_size\n\n        def forward(self, x):\n            # x shape: (batch, sequence_length, embed_size)\n            # In real attention, we compute how much each token should\n            # pay attention to every other token\n            print(f\"   Input shape: {x.shape}\")\n            print(f\"   (batch_size, sequence_length, embedding_dim)\")\n            return x  # Simplified for demo\n\n    # Example\n    print(\"\\nüìä Example: Processing 'Hello world'\\n\")\n\n    batch_size = 1  # Processing one sentence\n    seq_length = 2  # \"Hello\" and \"world\" (2 tokens)\n    embed_dim = 768  # Each token becomes 768 numbers\n\n    # Simulate token embeddings (use CPU if GPU not available)\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    token_embeddings = torch.randn(batch_size, seq_length, embed_dim, device=device)\n\n    print(f\"We have {seq_length} tokens\")\n    print(f\"Each token is represented by {embed_dim} numbers\")\n    print(f\"Device: {device}\")\n    print(f\"\\nTensor shape: {token_embeddings.shape}\")\n\n    # Apply attention\n    attention = SimpleAttention(embed_dim).to(device)\n    output = attention(token_embeddings)\n\n    print(f\"\\nAfter attention, shape: {output.shape}\")\n    print(\"\\nüí° In real GPT, this goes through many attention layers!\")\n\nexcept Exception as e:\n    print(f\"Error: {e}\")\n    print(f\"\\nConcept: Each word becomes a list of 768 numbers,\")\n    print(f\"and attention helps words understand their context!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 8: The Training Process\n",
    "\n",
    "## üéì Teaching the AI\n",
    "\n",
    "Training is where the model learns! Let's understand how.\n",
    "\n",
    "### The Learning Loop:\n",
    "\n",
    "```python\n",
    "for each batch of text:\n",
    "    1. Feed text into model\n",
    "    2. Model predicts next word\n",
    "    3. Compare prediction to actual next word\n",
    "    4. Calculate error (loss)\n",
    "    5. Adjust model parameters to reduce error\n",
    "    6. Repeat billions of times!\n",
    "```\n",
    "\n",
    "### The Math Behind Learning:\n",
    "\n",
    "1. **Forward Pass:** Input ‚Üí Model ‚Üí Prediction\n",
    "2. **Loss Calculation:** How wrong is the prediction?\n",
    "3. **Backward Pass:** Calculate what changes would improve the prediction\n",
    "4. **Parameter Update:** Actually make those changes\n",
    "\n",
    "This is called **Backpropagation** and **Gradient Descent**!\n",
    "\n",
    "### What the Model is Learning:\n",
    "\n",
    "- Grammar rules (\"I am\" not \"I are\")\n",
    "- Facts (\"Paris is the capital of France\")\n",
    "- Patterns (code syntax, story structure)\n",
    "- Context (\"bank\" = river bank or money bank?)\n",
    "- Reasoning (cause and effect)\n",
    "\n",
    "Let's look at the training script!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Let's examine the speedrun training script\nprint(\"üöÄ The Speedrun Training Pipeline\\n\")\nprint(\"=\"*60)\n\nfrom pathlib import Path\n\nspeedrun_script = Path(\"/var/www/gpt2/nanochat/speedrun.sh\")\n\nif speedrun_script.exists():\n    with open(speedrun_script, 'r') as f:\n        script = f.read()\n\n    # Show first 20 lines of the script\n    print(\"\\nüìÑ speedrun.sh (first 20 lines):\\n\")\n    lines = script.split('\\n')[:20]\n    for line in lines:\n        print(f\"   {line}\")\n    print(\"   ...\")\nelse:\n    print(f\"‚ö†Ô∏è  speedrun.sh not found at {speedrun_script}\")\n\n# Extract and explain key sections\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nüìã Training Pipeline Overview:\\n\")\n\nstages = [\n    (\"TOKENIZER\", \"Training the tokenizer to break text into pieces\"),\n    (\"Base model\", \"Learning to predict next words from web text\"),\n    (\"Midtraining\", \"Learning conversation format and tools\"),\n    (\"Supervised Finetuning\", \"Learning from high-quality examples\"),\n    (\"Reinforcement Learning\", \"Getting better at specific tasks\")\n]\n\nfor i, (stage, description) in enumerate(stages, 1):\n    print(f\"\\n{i}. {stage}\")\n    print(f\"   ‚Üí {description}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\n‚è±Ô∏è  Timeline (on your GB10 GPU):\")\nprint(\"   Total time: ~4 hours\")\nprint(\"   Total cost: ~$100 in compute\")\nprint(\"   Result: Your own ChatGPT!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a simplified training step\n",
    "print(\"üî¨ Simulating One Training Step\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    print(\"\\nExample: Teaching the model that 'The sky is ___'\\n\")\n",
    "    \n",
    "    # Simulate model predictions (probabilities for next word)\n",
    "    vocab_size = 5  # Simplified: only 5 possible words\n",
    "    words = [\"blue\", \"green\", \"red\", \"yellow\", \"purple\"]\n",
    "    \n",
    "    # Initial prediction (random, before training)\n",
    "    initial_logits = torch.randn(vocab_size)\n",
    "    initial_probs = F.softmax(initial_logits, dim=0)\n",
    "    \n",
    "    print(\"‚ùå BEFORE TRAINING:\")\n",
    "    for word, prob in zip(words, initial_probs):\n",
    "        bar = '‚ñà' * int(prob * 50)\n",
    "        print(f\"   {word:8s}: {bar} {prob*100:.1f}%\")\n",
    "    \n",
    "    # The correct answer\n",
    "    correct_answer = 0  # \"blue\"\n",
    "    \n",
    "    # After training (model learns \"blue\" is correct)\n",
    "    trained_logits = torch.tensor([2.0, -1.0, -1.0, -0.5, -1.0])  # Favor \"blue\"\n",
    "    trained_probs = F.softmax(trained_logits, dim=0)\n",
    "    \n",
    "    print(\"\\n‚úÖ AFTER TRAINING:\")\n",
    "    for word, prob in zip(words, trained_probs):\n",
    "        bar = '‚ñà' * int(prob * 50)\n",
    "        print(f\"   {word:8s}: {bar} {prob*100:.1f}%\")\n",
    "    \n",
    "    print(\"\\nüí° See how training increased the probability of 'blue'!\")\n",
    "    print(\"   This happens billions of times with millions of examples.\")\n",
    "    \n",
    "except:\n",
    "    print(\"Concept: The model starts with random guesses and gradually\")\n",
    "    print(\"learns to predict the correct next word through training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 9: Evaluation - How Smart is Our AI?\n",
    "\n",
    "## üìä Testing the Model\n",
    "\n",
    "After training, we need to test if our model actually learned anything! nanochat uses several benchmarks:\n",
    "\n",
    "### 1. **ARC (AI2 Reasoning Challenge)**\n",
    "Science questions that require reasoning\n",
    "- Example: \"What happens when you heat ice?\"\n",
    "- Tests: Common sense, science knowledge\n",
    "\n",
    "### 2. **GSM8K (Grade School Math)**\n",
    "Math word problems\n",
    "- Example: \"If John has 5 apples and buys 3 more, how many does he have?\"\n",
    "- Tests: Math reasoning, step-by-step thinking\n",
    "\n",
    "### 3. **MMLU (Massive Multitask Language Understanding)**\n",
    "Multiple choice questions across many topics\n",
    "- Topics: History, science, literature, law, etc.\n",
    "- Tests: General knowledge\n",
    "\n",
    "### 4. **HumanEval**\n",
    "Python coding problems\n",
    "- Example: \"Write a function that checks if a number is prime\"\n",
    "- Tests: Programming ability\n",
    "\n",
    "### 5. **CORE Score**\n",
    "Tests the base model's language understanding before it learns to chat\n",
    "\n",
    "### Example Results:\n",
    "For the $100 speedrun model:\n",
    "- ARC-Challenge: ~28% correct (vs 25% random guessing)\n",
    "- GSM8K: ~4-7% correct (math is hard!)\n",
    "- MMLU: ~31% correct (better than random!)\n",
    "- HumanEval: ~8% correct (can write simple code)\n",
    "\n",
    "Compare to GPT-4:\n",
    "- ARC-Challenge: ~96%\n",
    "- GSM8K: ~92%\n",
    "- MMLU: ~86%\n",
    "- HumanEval: ~67%\n",
    "\n",
    "Our model is like a kindergartener - it knows some things, but makes a lot of mistakes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 10: Making the Model Chat\n",
    "\n",
    "## üí¨ From Predictor to Chatbot\n",
    "\n",
    "Now comes the cool part - turning our next-word predictor into a chatbot!\n",
    "\n",
    "### The Secret: Clever Formatting\n",
    "\n",
    "We teach the model a special format:\n",
    "```\n",
    "<|user|>Your question here<|end|>\n",
    "<|assistant|>Model's response here<|end|>\n",
    "```\n",
    "\n",
    "### How It Works:\n",
    "1. User types: \"Why is the sky blue?\"\n",
    "2. We format it: `<|user|>Why is the sky blue?<|end|><|assistant|>`\n",
    "3. Model predicts what comes next (the answer!)\n",
    "4. Model generates: \"The sky appears blue because...\"\n",
    "5. Model generates `<|end|>` when done\n",
    "6. We show the user just the answer part\n",
    "\n",
    "### Generation Strategies:\n",
    "\n",
    "**1. Greedy Decoding:** Always pick the most likely word\n",
    "- Pro: Consistent\n",
    "- Con: Boring, repetitive\n",
    "\n",
    "**2. Temperature Sampling:** Add some randomness\n",
    "- Temperature = 0: Always pick most likely (greedy)\n",
    "- Temperature = 1: Natural randomness\n",
    "- Temperature = 2: Very creative (sometimes too random!)\n",
    "\n",
    "**3. Top-K Sampling:** Only consider top K most likely words\n",
    "- Prevents choosing unlikely/nonsense words\n",
    "- Keeps generation sensible\n",
    "\n",
    "**4. Top-P (Nucleus) Sampling:** Choose from smallest set of words whose probabilities add to P\n",
    "- Adaptive: sometimes many words, sometimes few\n",
    "- Most natural-sounding generation\n",
    "\n",
    "nanochat uses Top-P sampling by default!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate different sampling strategies\n",
    "print(\"üé≤ Text Generation Strategies\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    # Simulate model predictions for next word\n",
    "    words = [\"great\", \"good\", \"wonderful\", \"amazing\", \"nice\", \"ok\", \"fine\"]\n",
    "    logits = torch.tensor([3.0, 2.5, 2.3, 2.0, 1.5, 1.0, 0.5])\n",
    "    \n",
    "    print(\"Context: 'Today was a ___'\\n\")\n",
    "    print(\"Model's predictions:\\n\")\n",
    "    \n",
    "    probs = F.softmax(logits, dim=0)\n",
    "    for word, prob in zip(words, probs):\n",
    "        bar = '‚ñà' * int(prob * 50)\n",
    "        print(f\"  {word:10s}: {bar} {prob*100:.1f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    \n",
    "    # Strategy 1: Greedy\n",
    "    print(\"\\n1Ô∏è‚É£  GREEDY (always pick most likely):\")\n",
    "    greedy_choice = words[torch.argmax(logits)]\n",
    "    print(f\"   Result: 'Today was a {greedy_choice}'\")\n",
    "    print(\"   Same every time! ‚úì Reliable ‚úó Boring\")\n",
    "    \n",
    "    # Strategy 2: Temperature = 0.5 (more focused)\n",
    "    print(\"\\n2Ô∏è‚É£  TEMPERATURE = 0.5 (more focused):\")\n",
    "    temp_logits = logits / 0.5\n",
    "    temp_probs = F.softmax(temp_logits, dim=0)\n",
    "    print(f\"   'great': {temp_probs[0]*100:.1f}% (was {probs[0]*100:.1f}%)\")\n",
    "    print(f\"   'ok': {temp_probs[-2]*100:.1f}% (was {probs[-2]*100:.1f}%)\")\n",
    "    print(\"   More confident! ‚úì Decisive ‚úó Less creative\")\n",
    "    \n",
    "    # Strategy 3: Temperature = 2.0 (more creative)\n",
    "    print(\"\\n3Ô∏è‚É£  TEMPERATURE = 2.0 (more creative):\")\n",
    "    temp_logits = logits / 2.0\n",
    "    temp_probs = F.softmax(temp_logits, dim=0)\n",
    "    print(f\"   'great': {temp_probs[0]*100:.1f}% (was {probs[0]*100:.1f}%)\")\n",
    "    print(f\"   'ok': {temp_probs[-2]*100:.1f}% (was {probs[-2]*100:.1f}%)\")\n",
    "    print(\"   More random! ‚úì Creative ‚úó Less reliable\")\n",
    "    \n",
    "    # Strategy 4: Top-K = 3\n",
    "    print(\"\\n4Ô∏è‚É£  TOP-K = 3 (only consider top 3):\")\n",
    "    top_k = 3\n",
    "    top_k_probs, top_k_indices = torch.topk(probs, top_k)\n",
    "    print(f\"   Only choosing from: {[words[i] for i in top_k_indices]}\")\n",
    "    print(\"   ‚úì Prevents nonsense ‚úì Still varied\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"\\nüí° nanochat uses Top-P (nucleus) sampling:\")\n",
    "    print(\"   It picks from the smallest set of words that\")\n",
    "    print(\"   covers P% probability (usually P=0.9 or 90%)\")\n",
    "    print(\"   This gives natural, coherent text!\")\n",
    "    \n",
    "except:\n",
    "    print(\"Concept: Different sampling strategies create different text styles!\")\n",
    "    print(\"Greedy = boring but safe, High temperature = creative but risky\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 11: Understanding the Full Architecture\n",
    "\n",
    "## üèõÔ∏è Putting It All Together\n",
    "\n",
    "Let's review the complete picture of how everything connects:\n",
    "\n",
    "### The Complete Flow:\n",
    "\n",
    "```\n",
    "USER INPUT\n",
    "    ‚Üì\n",
    "\"Why is the sky blue?\"\n",
    "    ‚Üì\n",
    "TOKENIZATION\n",
    "    ‚Üì\n",
    "[\"Why\", \" is\", \" the\", \" sky\", \" blue\", \"?\"]\n",
    "    ‚Üì\n",
    "TOKEN IDs\n",
    "    ‚Üì\n",
    "[5195, 318, 262, 6766, 4171, 30]\n",
    "    ‚Üì\n",
    "EMBEDDINGS (convert to 768-dimensional vectors)\n",
    "    ‚Üì\n",
    "[[0.23, -0.45, 0.12, ...], [0.56, 0.23, -0.11, ...], ...]\n",
    "    ‚Üì\n",
    "LAYER 1: Self-Attention + Feed-Forward\n",
    "    ‚Üì (each layer adds understanding)\n",
    "LAYER 2: Self-Attention + Feed-Forward\n",
    "    ‚Üì\n",
    "    ...\n",
    "    ‚Üì\n",
    "LAYER 20: Self-Attention + Feed-Forward\n",
    "    ‚Üì\n",
    "FINAL LAYER OUTPUT (rich representations)\n",
    "    ‚Üì\n",
    "PREDICTION HEAD (convert to vocab probabilities)\n",
    "    ‚Üì\n",
    "[\"The\": 45%, \"Blue\": 12%, \"A\": 8%, ...]\n",
    "    ‚Üì\n",
    "SAMPLING (pick next word with Top-P)\n",
    "    ‚Üì\n",
    "\"The\"\n",
    "    ‚Üì\n",
    "ADD TO SEQUENCE, REPEAT\n",
    "    ‚Üì\n",
    "\"The sky appears blue because...\"\n",
    "    ‚Üì\n",
    "FINAL RESPONSE\n",
    "```\n",
    "\n",
    "### Key Innovations That Make This Work:\n",
    "\n",
    "1. **Attention Mechanism** (2017): Let words understand context\n",
    "2. **Large Scale Training** (2018-2020): Train on massive datasets\n",
    "3. **In-Context Learning** (2020): Model learns from examples without retraining\n",
    "4. **Instruction Tuning** (2022): Make models helpful and safe\n",
    "5. **RLHF** (2022): Reinforcement Learning from Human Feedback\n",
    "\n",
    "### Why It's Called \"Generative Pre-trained Transformer\" (GPT):\n",
    "\n",
    "- **Generative**: Generates new text\n",
    "- **Pre-trained**: Trained on lots of data first\n",
    "- **Transformer**: Uses the transformer architecture (attention!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 12: Limitations and Future\n",
    "\n",
    "## ‚ö†Ô∏è What Our Model Can't Do (Yet!)\n",
    "\n",
    "### Current Limitations:\n",
    "\n",
    "1. **Hallucinations**: Makes up facts confidently\n",
    "   - Example: Might say \"Paris is in Germany\" with confidence!\n",
    "   - Why: Learned to sound confident, not to verify truth\n",
    "\n",
    "2. **No Real Understanding**: Pattern matching, not true comprehension\n",
    "   - Example: Can't truly \"understand\" what it means to be happy\n",
    "   - Why: No consciousness or lived experience\n",
    "\n",
    "3. **Math Difficulties**: Struggles with arithmetic\n",
    "   - Example: May get 234 √ó 876 wrong\n",
    "   - Why: Trained to predict text, not calculate\n",
    "\n",
    "4. **Knowledge Cutoff**: Doesn't know events after training\n",
    "   - Our model: Knows data up to training date only\n",
    "   - Why: Frozen after training\n",
    "\n",
    "5. **Context Length**: Can only \"remember\" ~1000 words\n",
    "   - Example: Forgets beginning of long conversations\n",
    "   - Why: Fixed context window (1024 tokens)\n",
    "\n",
    "6. **Reasoning Limits**: Better at pattern matching than logic\n",
    "   - Example: Can struggle with complex multi-step problems\n",
    "   - Why: Predicts next token, doesn't \"think ahead\"\n",
    "\n",
    "### The Gap to GPT-4:\n",
    "\n",
    "Our $100 model vs GPT-4:\n",
    "- **Size**: 560M parameters vs ~1 trillion+\n",
    "- **Training**: $100 vs ~$100 million\n",
    "- **Data**: 11B tokens vs ~10 trillion+\n",
    "- **Time**: 4 hours vs months\n",
    "\n",
    "It's like comparing a kindergartener to a PhD!\n",
    "\n",
    "### Active Areas of Research:\n",
    "\n",
    "1. **Reducing Hallucinations**: Making models more truthful\n",
    "2. **Longer Context**: Remembering more (up to millions of tokens!)\n",
    "3. **Multi-modal**: Understanding images, audio, video\n",
    "4. **Efficiency**: Making models faster and cheaper\n",
    "5. **Reasoning**: Better logic and planning\n",
    "6. **Tool Use**: Calling calculators, databases, APIs\n",
    "\n",
    "### What You Can Do:\n",
    "\n",
    "- **Experiment**: Try different model sizes and datasets\n",
    "- **Customize**: Give your model a unique personality\n",
    "- **Evaluate**: Test on your own benchmarks\n",
    "- **Contribute**: Help improve nanochat!\n",
    "- **Learn**: This is just the beginning of your AI journey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 13: Hands-On Challenge\n",
    "\n",
    "## üéØ Try It Yourself!\n",
    "\n",
    "Now that you understand how it all works, here are some challenges:\n",
    "\n",
    "### Beginner Challenges:\n",
    "\n",
    "1. **Explore the Code**: \n",
    "   - Read through `nanochat/gpt.py`\n",
    "   - Try to understand each component\n",
    "   - Draw diagrams of the architecture\n",
    "\n",
    "2. **Tokenization Experiments**:\n",
    "   - Try tokenizing different languages\n",
    "   - Compare efficiency (tokens per character)\n",
    "   - See how emojis are tokenized!\n",
    "\n",
    "3. **Change the Config**:\n",
    "   - What happens with 12 layers vs 20?\n",
    "   - Try different embedding sizes\n",
    "   - Experiment with number of attention heads\n",
    "\n",
    "### Intermediate Challenges:\n",
    "\n",
    "4. **Custom Dataset**:\n",
    "   - Collect text from your favorite books/websites\n",
    "   - Train on domain-specific data\n",
    "   - Create a specialized model!\n",
    "\n",
    "5. **Personality Tuning**:\n",
    "   - Follow the identity guide in nanochat docs\n",
    "   - Give your model a unique personality\n",
    "   - Make it an expert in a specific topic\n",
    "\n",
    "6. **New Evaluation**:\n",
    "   - Create your own benchmark\n",
    "   - Test on questions you care about\n",
    "   - Compare different model versions\n",
    "\n",
    "### Advanced Challenges:\n",
    "\n",
    "7. **Architecture Modifications**:\n",
    "   - Try different attention patterns\n",
    "   - Experiment with layer configurations\n",
    "   - Add new features to the model\n",
    "\n",
    "8. **Training Optimization**:\n",
    "   - Tune hyperparameters (learning rate, batch size)\n",
    "   - Try different optimizers\n",
    "   - Improve training speed\n",
    "\n",
    "9. **Build Something New**:\n",
    "   - Code completion tool\n",
    "   - Story generator\n",
    "   - Question-answering system\n",
    "   - Educational tutor\n",
    "\n",
    "### Research Ideas:\n",
    "\n",
    "10. **Investigate Questions**:\n",
    "    - How does model size affect performance?\n",
    "    - What's the optimal data amount?\n",
    "    - Can we make training more efficient?\n",
    "    - How do different tasks affect each other?\n",
    "\n",
    "Pick one and start experimenting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge starter: Let's explore model configurations\n",
    "print(\"üéÆ Model Configuration Playground\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def calculate_model_size(n_layer, n_embd, vocab_size, n_head):\n",
    "    \"\"\"\n",
    "    Calculate approximate number of parameters in the model.\n",
    "    This is educational - the actual calculation is more complex!\n",
    "    \"\"\"\n",
    "    # Embedding layer\n",
    "    embedding_params = vocab_size * n_embd\n",
    "    \n",
    "    # Each transformer layer has:\n",
    "    # - Attention: 4 * n_embd^2 (Q, K, V, projection)\n",
    "    # - Feed-forward: 8 * n_embd^2 (typically 4x expansion)\n",
    "    params_per_layer = 4 * n_embd * n_embd + 8 * n_embd * n_embd\n",
    "    transformer_params = n_layer * params_per_layer\n",
    "    \n",
    "    # Output layer\n",
    "    output_params = vocab_size * n_embd\n",
    "    \n",
    "    total = embedding_params + transformer_params + output_params\n",
    "    return total\n",
    "\n",
    "# Example configurations\n",
    "configs = [\n",
    "    (\"Tiny (for testing)\", 6, 384, 50304, 6),\n",
    "    (\"Small (GPT-2 small)\", 12, 768, 50304, 12),\n",
    "    (\"nanochat d20\", 20, 768, 65536, 6),\n",
    "    (\"nanochat d26\", 26, 768, 65536, 6),\n",
    "    (\"nanochat d34\", 34, 768, 65536, 6),\n",
    "]\n",
    "\n",
    "print(\"\\nüìä Model Configurations:\\n\")\n",
    "print(f\"{'Name':<20} {'Layers':<8} {'Embed':<8} {'Vocab':<10} {'Heads':<8} {'Parameters'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, n_layer, n_embd, vocab_size, n_head in configs:\n",
    "    params = calculate_model_size(n_layer, n_embd, vocab_size, n_head)\n",
    "    params_m = params / 1_000_000\n",
    "    print(f\"{name:<20} {n_layer:<8} {n_embd:<8} {vocab_size:<10} {n_head:<8} {params_m:.0f}M\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nüéØ Your turn!\")\n",
    "print(\"Try different configurations and see how the size changes.\")\n",
    "print(\"Remember: More parameters = smarter but slower and more expensive!\")\n",
    "\n",
    "# Interactive calculator\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"\\nüßÆ Try your own configuration:\\n\")\n",
    "print(\"Example: calculate_model_size(n_layer=15, n_embd=512, vocab_size=50000, n_head=8)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Part 14: Running the Full Pipeline on Your GB10 GPU\n\n## ‚ö° The Complete Speedrun\n\nYou have access to the GB10 Blackwell GPU (128GB VRAM) - perfect for training!\n\n### Your Setup:\n- **GPU**: GB10 Blackwell (128GB VRAM)\n- **Location**: /var/www/gpt2/nanochat\n- **Time**: ~4 hours for speedrun\n- **Cost**: ~$100 in compute\n\n### Training Steps:\n\n**Option 1: Run in Terminal** (Recommended)\n\nOpen a terminal/SSH session:\n\n```bash\n# Navigate to nanochat\ncd /var/www/gpt2/nanochat\n\n# Activate the virtual environment\nsource .venv/bin/activate\n\n# Optional: Set up wandb for tracking\nwandb login\n\n# Run the complete pipeline\nbash speedrun.sh\n\n# OR with wandb tracking:\nWANDB_RUN=my_first_llm bash speedrun.sh\n\n# OR in a screen session (best for long training):\nscreen -L -Logfile speedrun.log -S speedrun bash speedrun.sh\n# (Detach with Ctrl+A then D, reattach with: screen -r speedrun)\n```\n\n**Option 2: Run from Notebook** (Not Recommended - use terminal instead)\n\n```python\nimport subprocess\nimport os\n\nos.chdir('/var/www/gpt2/nanochat')\nsubprocess.run(['bash', 'speedrun.sh'])\n```\n\n### What Happens:\n\n1. **Downloads data** (~40GB of web text)\n2. **Trains tokenizer** (10 mins)\n3. **Pretrains model** (2-3 hours) - the longest part!\n4. **Evaluates base model** (15 mins)\n5. **Midtraining** (20 mins)\n6. **Supervised fine-tuning** (15 mins)\n7. **Evaluates final model** (15 mins)\n8. **Generates report** (1 min)\n\n### After Training:\n\n```bash\n# Chat with your model in terminal\ncd /var/www/gpt2/nanochat\nsource .venv/bin/activate\npython -m scripts.chat_cli\n\n# OR use the web interface\npython -m scripts.chat_web\n# Then open http://192.168.219.45:8000 in your browser\n```\n\n### The Report:\n\nAfter training, you'll get a `report.md` file at `/var/www/gpt2/nanochat/report.md` with:\n- All evaluation scores\n- Sample outputs\n- Training statistics\n- Comparison to baselines\n\n### Training Options:\n- **Speedrun (d20)**: ~$100, 4 hours\n- **Mid-tier (d26)**: ~$300, 12 hours\n- **Full-tier (d34)**: ~$1000, 41 hours\n\nTo run different sizes:\n```bash\n# Mid-tier\nbash run1000.sh\n\n# Custom configuration\npython -m scripts.base_train --depth=26 --device-batch-size=32\n```\n\n### Monitoring Training:\n\n```bash\n# Watch GPU usage\nwatch -n 1 nvidia-smi\n\n# Tail the training logs\ntail -f ~/.cache/nanochat/logs/training.log\n\n# Check wandb dashboard (if enabled)\n# Visit: https://wandb.ai\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 15: Going Deeper - Resources and Next Steps\n",
    "\n",
    "## üìö Continue Your Learning Journey\n",
    "\n",
    "Congratulations! You now understand how ChatGPT-like models work. Here's where to go next:\n",
    "\n",
    "### Essential Resources:\n",
    "\n",
    "#### 1. **Courses:**\n",
    "- [Andrej Karpathy's Neural Networks: Zero to Hero](https://karpathy.ai/zero-to-hero.html)\n",
    "  - Free YouTube series\n",
    "  - Builds neural networks from scratch\n",
    "  - Start here if you want deeper understanding!\n",
    "\n",
    "- [Fast.ai - Practical Deep Learning](https://course.fast.ai/)\n",
    "  - Free online course\n",
    "  - Very practical approach\n",
    "  - Great for getting started quickly\n",
    "\n",
    "- [Stanford CS224N: Natural Language Processing](https://web.stanford.edu/class/cs224n/)\n",
    "  - University-level course\n",
    "  - Lectures free on YouTube\n",
    "  - More theoretical\n",
    "\n",
    "#### 2. **Papers to Read:**\n",
    "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (2017)\n",
    "  - The original Transformer paper\n",
    "  - Changed everything!\n",
    "\n",
    "- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) (2020)\n",
    "  - The GPT-3 paper\n",
    "  - Shows power of scale\n",
    "\n",
    "- [Training language models to follow instructions](https://arxiv.org/abs/2203.02155) (2022)\n",
    "  - InstructGPT / ChatGPT techniques\n",
    "  - How to make models helpful\n",
    "\n",
    "#### 3. **Interactive Resources:**\n",
    "- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n",
    "  - Visual guide to transformers\n",
    "  - Great for visual learners\n",
    "\n",
    "- [Transformer Explainer](https://poloclub.github.io/transformer-explainer/)\n",
    "  - Interactive visualization\n",
    "  - See attention in action!\n",
    "\n",
    "- [LLM Visualization](https://bbycroft.net/llm)\n",
    "  - 3D interactive model\n",
    "  - Explore layer by layer\n",
    "\n",
    "#### 4. **Code Projects:**\n",
    "- [nanoGPT](https://github.com/karpathy/nanoGPT)\n",
    "  - nanochat's predecessor\n",
    "  - Simpler, just pretraining\n",
    "\n",
    "- [minGPT](https://github.com/karpathy/minGPT)\n",
    "  - Even simpler educational implementation\n",
    "  - Great for learning basics\n",
    "\n",
    "- [transformers by HuggingFace](https://github.com/huggingface/transformers)\n",
    "  - Production-ready library\n",
    "  - Use pre-trained models\n",
    "\n",
    "### Topics to Explore:\n",
    "\n",
    "1. **Attention Mechanisms**\n",
    "   - Self-attention\n",
    "   - Multi-head attention\n",
    "   - Grouped-query attention\n",
    "   - Flash attention\n",
    "\n",
    "2. **Training Techniques**\n",
    "   - Gradient descent & backpropagation\n",
    "   - Optimizers (AdamW, Muon)\n",
    "   - Learning rate schedules\n",
    "   - Mixed precision training\n",
    "\n",
    "3. **Advanced Topics**\n",
    "   - RLHF (Reinforcement Learning from Human Feedback)\n",
    "   - Scaling laws\n",
    "   - Model compression\n",
    "   - Prompt engineering\n",
    "   - Fine-tuning techniques\n",
    "\n",
    "4. **Ethics & Safety**\n",
    "   - Bias in AI\n",
    "   - Responsible AI development\n",
    "   - AI alignment\n",
    "   - Privacy considerations\n",
    "\n",
    "### Career Paths:\n",
    "\n",
    "If you're interested in AI professionally:\n",
    "- **Machine Learning Engineer**: Build and deploy AI systems\n",
    "- **Research Scientist**: Advance the field with new techniques\n",
    "- **AI Safety Researcher**: Make AI safe and beneficial\n",
    "- **MLOps Engineer**: Infrastructure for AI systems\n",
    "- **Data Scientist**: Use AI to solve business problems\n",
    "\n",
    "### Keep Experimenting!\n",
    "\n",
    "The best way to learn is by doing:\n",
    "1. Modify nanochat\n",
    "2. Train your own models\n",
    "3. Try new ideas\n",
    "4. Break things (you'll learn the most!)\n",
    "5. Share your findings\n",
    "\n",
    "### Join the Community:\n",
    "\n",
    "- [nanochat Discussions](https://github.com/karpathy/nanochat/discussions)\n",
    "- [r/MachineLearning](https://reddit.com/r/MachineLearning)\n",
    "- [Hugging Face Forums](https://discuss.huggingface.co/)\n",
    "- AI Discord servers and communities\n",
    "\n",
    "Remember: Every expert was once a beginner. Keep learning, stay curious, and have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 16: Summary & Key Takeaways\n",
    "\n",
    "## üéì What You've Learned\n",
    "\n",
    "### Core Concepts:\n",
    "\n",
    "1. **Language Models are Next-Word Predictors**\n",
    "   - They learn patterns from vast amounts of text\n",
    "   - Generate text by predicting one token at a time\n",
    "   - \"Understanding\" is pattern matching, not consciousness\n",
    "\n",
    "2. **The Transformer Architecture**\n",
    "   - Attention mechanism: words understand context\n",
    "   - Embeddings: convert text to meaningful numbers\n",
    "   - Layers: stack processing for deeper understanding\n",
    "   - ~560M to 2B parameters store knowledge\n",
    "\n",
    "3. **The Training Pipeline**\n",
    "   - Tokenization: teach the model to read\n",
    "   - Pretraining: learn language from web text\n",
    "   - Midtraining: learn conversation format\n",
    "   - Supervised fine-tuning: learn from good examples\n",
    "   - Reinforcement learning: optimize for specific tasks\n",
    "\n",
    "4. **From Predictor to Chatbot**\n",
    "   - Special formatting: `<|user|>...<|assistant|>...`\n",
    "   - Sampling strategies control creativity\n",
    "   - Top-P sampling produces natural text\n",
    "\n",
    "5. **Evaluation Matters**\n",
    "   - Multiple benchmarks test different abilities\n",
    "   - Our $100 model is like a kindergartener\n",
    "   - GPT-4 is like a PhD student\n",
    "   - The gap is in scale: data, compute, parameters\n",
    "\n",
    "### The Big Picture:\n",
    "\n",
    "```\n",
    "Massive Text Data\n",
    "        +\n",
    "Transformer Architecture (Attention!)\n",
    "        +\n",
    "Powerful GPUs & Lots of Time\n",
    "        +\n",
    "Smart Training Techniques\n",
    "        ‚Üì\n",
    "ChatGPT-like AI Assistant!\n",
    "```\n",
    "\n",
    "### What Makes Modern LLMs Work:\n",
    "\n",
    "1. **Scale**: Bigger models, more data, more compute\n",
    "2. **Attention**: Words can look at all other words\n",
    "3. **Unsupervised Learning**: Learn from raw text\n",
    "4. **Transfer Learning**: Pre-train once, fine-tune for tasks\n",
    "5. **Human Feedback**: RLHF makes models helpful\n",
    "\n",
    "### Important Limitations:\n",
    "\n",
    "- ‚ùå No true understanding or consciousness\n",
    "- ‚ùå Can't access real-time information\n",
    "- ‚ùå Makes confident mistakes (hallucinations)\n",
    "- ‚ùå Limited context window\n",
    "- ‚ùå Struggles with logic and math\n",
    "- ‚ùå Reflects biases in training data\n",
    "\n",
    "### But Also:\n",
    "\n",
    "- ‚úÖ Amazing at pattern recognition\n",
    "- ‚úÖ Can help with many tasks\n",
    "- ‚úÖ Learns from few examples\n",
    "- ‚úÖ Accessible to individuals (via nanochat!)\n",
    "- ‚úÖ Improving rapidly\n",
    "- ‚úÖ Open source versions available\n",
    "\n",
    "### Your Next Steps:\n",
    "\n",
    "1. **Explore the nanochat code** - read and understand each file\n",
    "2. **Experiment** - change configurations, try new ideas\n",
    "3. **Learn the fundamentals** - take courses, read papers\n",
    "4. **Build something** - create your own AI project\n",
    "5. **Join the community** - share and learn from others\n",
    "6. **Stay ethical** - consider the impact of AI systems\n",
    "\n",
    "### Final Thoughts:\n",
    "\n",
    "You now understand more about AI than 99% of people! The technology is fascinating, powerful, and rapidly evolving. But remember:\n",
    "\n",
    "- **AI is a tool**, not magic\n",
    "- **Understanding matters** - don't just use black boxes\n",
    "- **Ethics matter** - build responsible AI\n",
    "- **Keep learning** - the field changes fast\n",
    "- **Have fun** - AI is amazing to work with!\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ You're Ready!\n",
    "\n",
    "You've completed this introduction to building ChatGPT from scratch. The world of AI is now open to you. Go forth and build amazing things!\n",
    "\n",
    "**Questions? Ideas? Discoveries?**\n",
    "Share them in the [nanochat Discussions](https://github.com/karpathy/nanochat/discussions)!\n",
    "\n",
    "---\n",
    "\n",
    "*Created with ‚ù§Ô∏è for curious learners everywhere*\n",
    "\n",
    "*Based on [nanochat](https://github.com/karpathy/nanochat) by Andrej Karpathy*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéâ Congratulations!\n",
    "\n",
    "You've completed the nanochat tutorial! You now understand:\n",
    "- How language models work\n",
    "- The transformer architecture\n",
    "- The complete training pipeline\n",
    "- How to build your own ChatGPT\n",
    "\n",
    "Keep exploring, keep learning, and most importantly - keep building! üöÄ"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}